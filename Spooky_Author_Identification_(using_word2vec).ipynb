{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spooky Author Identification-(using word2vec).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "WuJXOtD81wT2",
        "colab_type": "code",
        "outputId": "caefad20-e4d1-4c2e-fa0c-d3f7aba0663c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4437
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all',halt_on_error=False)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "zfw8Z4ul3u4z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Ignore  the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('always')\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# data visualisation and manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        "#configure\n",
        "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
        "% matplotlib inline  \n",
        "style.use('fivethirtyeight')\n",
        "sns.set(style='whitegrid',color_codes=True)\n",
        "\n",
        "#nltk\n",
        "import nltk\n",
        "\n",
        "#preprocessing\n",
        "from nltk.corpus import stopwords  #stopwords\n",
        "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
        "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
        "\n",
        "# for part-of-speech tagging\n",
        "from nltk import pos_tag\n",
        "\n",
        "# for named entity recognition (NER)\n",
        "from nltk import ne_chunk\n",
        "\n",
        "# vectorizers for creating the document-term-matrix (DTM)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "\n",
        "#model_selection\n",
        "from sklearn.model_selection import train_test_split,cross_validate\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#evaluation\n",
        "from sklearn.metrics import accuracy_score,roc_auc_score \n",
        "from sklearn.metrics import classification_report\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "#preprocessing scikit\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler,Imputer,LabelEncoder\n",
        "\n",
        "#modelling algos\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "#gensim w2v\n",
        "#word2vec\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "#stop-words\n",
        "stop_words=set(nltk.corpus.stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GIlYaX_A3u7s",
        "colab_type": "code",
        "outputId": "3a6729ab-cfa3-4436-a65a-066bf9cc2620",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mPnmnPAp4B2l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d228ae1e-1622-45df-c7cc-c45e96638406"
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fuse: mountpoint is not empty\n",
            "fuse: if you are sure this is safe, use the 'nonempty' mount option\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BEFm2Mv34B5e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train=pd.read_csv(r'drive/Colab Notebooks/spooky author/train.csv')\n",
        "test=pd.read_csv(r'drive/Colab Notebooks/spooky author/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lGnshUjM5-Xb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df=train.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ucu7zq3e6BEt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "99553c18-9800-4e85-dc2b-805972a48f06"
      },
      "cell_type": "code",
      "source": [
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19579, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id26305</td>\n",
              "      <td>This process, however, afforded me no means of...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id17569</td>\n",
              "      <td>It never once occurred to me that the fumbling...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id11008</td>\n",
              "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id27763</td>\n",
              "      <td>How lovely is spring As we looked from Windsor...</td>\n",
              "      <td>MWS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id12958</td>\n",
              "      <td>Finding nothing else, not even gold, the Super...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                               text author\n",
              "0  id26305  This process, however, afforded me no means of...    EAP\n",
              "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
              "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
              "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
              "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "5pCHwQrk6GqH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "aeb2b3ad-5184-490a-bffd-741791d38667"
      },
      "cell_type": "code",
      "source": [
        "print(df.author.isnull().sum())  # no null values\n",
        "df.text.isnull().sum()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "8wF0hkHm6ahb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "de6a42e7-19c8-4b74-92a0-34566aea9b6f"
      },
      "cell_type": "code",
      "source": [
        "print(test.shape)\n",
        "test.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8392, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id02310</td>\n",
              "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id24541</td>\n",
              "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id00134</td>\n",
              "      <td>And when they had broken down the frail door t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id27757</td>\n",
              "      <td>While I was thinking how I should possibly man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id04081</td>\n",
              "      <td>I am not sure to what limit his knowledge may ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                               text\n",
              "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
              "1  id24541  If a fire wanted fanning, it could readily be ...\n",
              "2  id00134  And when they had broken down the frail door t...\n",
              "3  id27757  While I was thinking how I should possibly man...\n",
              "4  id04081  I am not sure to what limit his knowledge may ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "ftquB6U-6qhw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37c89cb3-74fe-4c34-9693-fb475cccb475"
      },
      "cell_type": "code",
      "source": [
        "# checking null values in test set.\n",
        "print(test.text.isnull().sum())  # no null values\n",
        " "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Mb9isVLA7c8s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5514b641-7a15-4849-f8d8-c09e944f5390"
      },
      "cell_type": "code",
      "source": [
        "# back to training set.\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id26305</td>\n",
              "      <td>This process, however, afforded me no means of...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id17569</td>\n",
              "      <td>It never once occurred to me that the fumbling...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id11008</td>\n",
              "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id27763</td>\n",
              "      <td>How lovely is spring As we looked from Windsor...</td>\n",
              "      <td>MWS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id12958</td>\n",
              "      <td>Finding nothing else, not even gold, the Super...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                               text author\n",
              "0  id26305  This process, however, afforded me no means of...    EAP\n",
              "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
              "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
              "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
              "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "cs2IFvKp7mLJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# drop the id column.\n",
        "df.drop(['id'],axis=1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qUnvWRjM8J4b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "42015e3f-ecb2-4324-85ad-e497e96235e5"
      },
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This process, however, afforded me no means of...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It never once occurred to me that the fumbling...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How lovely is spring As we looked from Windsor...</td>\n",
              "      <td>MWS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Finding nothing else, not even gold, the Super...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>A youth passed in solitude, my best years spen...</td>\n",
              "      <td>MWS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>The astronomer, perhaps, at this point, took r...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>The surcingle hung in ribands from my body.</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>I knew that you could not say to yourself 'ste...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>I confess that neither the structure of langua...</td>\n",
              "      <td>MWS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text author\n",
              "0  This process, however, afforded me no means of...    EAP\n",
              "1  It never once occurred to me that the fumbling...    HPL\n",
              "2  In his left hand was a gold snuff box, from wh...    EAP\n",
              "3  How lovely is spring As we looked from Windsor...    MWS\n",
              "4  Finding nothing else, not even gold, the Super...    HPL\n",
              "5  A youth passed in solitude, my best years spen...    MWS\n",
              "6  The astronomer, perhaps, at this point, took r...    EAP\n",
              "7        The surcingle hung in ribands from my body.    EAP\n",
              "8  I knew that you could not say to yourself 'ste...    EAP\n",
              "9  I confess that neither the structure of langua...    MWS"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "iBJR5Jv28L6k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# exploring.\n",
        "'''\n",
        "\n",
        "let us now see some worko f each author and see if we can figure out any patterns in their writing.\n",
        "\n",
        "'''\n",
        "\n",
        "eap_df=df.loc[df.author=='EAP','text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZMJKqJeQ88w1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "a2b65367-b8b6-4cf1-da48-57647a80062a"
      },
      "cell_type": "code",
      "source": [
        "print(\"Author EAP:\",\"\\n\")\n",
        "for i,text in enumerate(eap_df):\n",
        "  print(text,\"\\n\")\n",
        "  if(i==4):                 # no conclusions.\n",
        "    break"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Author EAP: \n",
            "\n",
            "This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall. \n",
            "\n",
            "In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction. \n",
            "\n",
            "The astronomer, perhaps, at this point, took refuge in the suggestion of non luminosity; and here analogy was suddenly let fall. \n",
            "\n",
            "The surcingle hung in ribands from my body. \n",
            "\n",
            "I knew that you could not say to yourself 'stereotomy' without being brought to think of atomies, and thus of the theories of Epicurus; and since, when we discussed this subject not very long ago, I mentioned to you how singularly, yet with how little notice, the vague guesses of that noble Greek had met with confirmation in the late nebular cosmogony, I felt that you could not avoid casting your eyes upward to the great nebula in Orion, and I certainly expected that you would do so. \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0rmba0EB9XaH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "2ee71294-9d0a-4509-9afa-3a5487dc3a10"
      },
      "cell_type": "code",
      "source": [
        "hpl_df=df.loc[df.author=='HPL','text']\n",
        "print(\"Author HPL:\",\"\\n\")\n",
        "for i,text in enumerate(hpl_df):\n",
        "  print(text,\"\\n\")\n",
        "  if(i==4):                 # no conclusions.\n",
        "    break"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Author HPL: \n",
            "\n",
            "It never once occurred to me that the fumbling might be a mere mistake. \n",
            "\n",
            "Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk. \n",
            "\n",
            "Herbert West needed fresh bodies because his life work was the reanimation of the dead. \n",
            "\n",
            "The farm like grounds extended back very deeply up the hill, almost to Wheaton Street. \n",
            "\n",
            "His facial aspect, too, was remarkable for its maturity; for though he shared his mother's and grandfather's chinlessness, his firm and precociously shaped nose united with the expression of his large, dark, almost Latin eyes to give him an air of quasi adulthood and well nigh preternatural intelligence. \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s7AUGwmz-_w-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "1a349f66-57ac-432a-944d-9943bf5bb2cd"
      },
      "cell_type": "code",
      "source": [
        "mws_df=df.loc[df.author=='MWS','text']\n",
        "print(\"Author MWS:\",\"\\n\")\n",
        "for i,text in enumerate(mws_df):\n",
        "  print(text,\"\\n\")\n",
        "  if(i==4):                 # no conclusions.\n",
        "    break"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Author MWS: \n",
            "\n",
            "How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair. \n",
            "\n",
            "A youth passed in solitude, my best years spent under your gentle and feminine fosterage, has so refined the groundwork of my character that I cannot overcome an intense distaste to the usual brutality exercised on board ship: I have never believed it to be necessary, and when I heard of a mariner equally noted for his kindliness of heart and the respect and obedience paid to him by his crew, I felt myself peculiarly fortunate in being able to secure his services. \n",
            "\n",
            "I confess that neither the structure of languages, nor the code of governments, nor the politics of various states possessed attractions for me. \n",
            "\n",
            "He shall find that I can feel my injuries; he shall learn to dread my revenge\" A few days after he arrived. \n",
            "\n",
            "He had escaped me, and I must commence a destructive and almost endless journey across the mountainous ices of the ocean, amidst cold that few of the inhabitants could long endure and which I, the native of a genial and sunny climate, could not hope to survive. \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pp2tRruJMCq3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# loading pre-trained google word2vec embeddings.\n",
        "from gensim.models import KeyedVectors\n",
        "w2v_model_pre = KeyedVectors.load_word2vec_format(r'drive/Colab Notebooks/spooky author/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M8vHrsXXkiXg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wAaAn9s9V8iu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1054
        },
        "outputId": "86b93472-b661-42ab-efe5-eaa662eff3cb"
      },
      "cell_type": "code",
      "source": [
        "# try to seek word embedding of a random word.\n",
        "w2v_model_pre.wv.get_vector('lovely')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.00060272,  0.06176758, -0.125     ,  0.171875  ,  0.171875  ,\n",
              "       -0.078125  ,  0.00787354, -0.02832031, -0.15039062,  0.26171875,\n",
              "       -0.17089844, -0.22167969, -0.09326172, -0.10107422,  0.17382812,\n",
              "        0.16308594,  0.08154297,  0.12695312, -0.11279297, -0.02868652,\n",
              "       -0.00421143,  0.16894531,  0.16308594, -0.08251953,  0.28515625,\n",
              "        0.17675781, -0.03955078,  0.11328125,  0.01409912, -0.18359375,\n",
              "       -0.17480469,  0.05981445,  0.24804688,  0.1875    , -0.03588867,\n",
              "       -0.29882812,  0.14257812, -0.12988281, -0.17871094,  0.15625   ,\n",
              "        0.04785156, -0.20605469,  0.34765625,  0.02587891,  0.21582031,\n",
              "       -0.07421875,  0.22363281,  0.02111816, -0.01519775, -0.02319336,\n",
              "        0.05029297,  0.07568359,  0.26367188,  0.15039062, -0.12060547,\n",
              "        0.00598145, -0.07666016, -0.08496094, -0.02001953, -0.01275635,\n",
              "       -0.18164062,  0.10644531, -0.20898438, -0.265625  ,  0.03759766,\n",
              "        0.08496094, -0.18457031, -0.08789062, -0.11083984,  0.32421875,\n",
              "        0.40820312,  0.06640625,  0.08496094,  0.06494141, -0.21679688,\n",
              "        0.23339844, -0.0859375 ,  0.05004883,  0.14941406,  0.1875    ,\n",
              "       -0.07617188,  0.01916504,  0.00582886, -0.12695312, -0.18164062,\n",
              "        0.08105469, -0.16113281,  0.15722656,  0.1328125 ,  0.11962891,\n",
              "       -0.02502441,  0.15820312, -0.23339844,  0.05053711, -0.265625  ,\n",
              "       -0.07861328, -0.03100586,  0.01263428, -0.04394531, -0.19140625,\n",
              "       -0.26953125,  0.03039551,  0.10351562, -0.00151062, -0.03955078,\n",
              "        0.05664062,  0.16113281,  0.0625    , -0.0402832 , -0.19140625,\n",
              "       -0.20703125,  0.17871094,  0.03857422,  0.00646973,  0.30859375,\n",
              "       -0.28125   ,  0.0859375 , -0.13964844,  0.15136719,  0.12207031,\n",
              "        0.05395508, -0.01531982,  0.2109375 , -0.0300293 , -0.13183594,\n",
              "        0.16601562, -0.20507812,  0.04150391,  0.11230469,  0.15625   ,\n",
              "       -0.22460938,  0.13574219,  0.01153564,  0.21972656,  0.02856445,\n",
              "        0.01531982, -0.13378906,  0.08544922,  0.07910156,  0.18359375,\n",
              "        0.06396484,  0.05151367,  0.00570679, -0.03930664, -0.05249023,\n",
              "        0.17773438, -0.12988281,  0.18261719, -0.21777344, -0.15917969,\n",
              "        0.17675781,  0.11181641,  0.04321289,  0.25585938, -0.11621094,\n",
              "       -0.10791016,  0.18457031,  0.00497437, -0.25195312, -0.08496094,\n",
              "       -0.13769531,  0.21582031,  0.24316406,  0.26953125,  0.18359375,\n",
              "       -0.27929688,  0.23046875, -0.16308594, -0.38476562,  0.09863281,\n",
              "       -0.09326172,  0.14550781,  0.01019287,  0.03344727,  0.05688477,\n",
              "        0.18457031,  0.11669922,  0.03198242, -0.06933594,  0.02148438,\n",
              "       -0.16796875, -0.11132812, -0.01867676, -0.0168457 , -0.09814453,\n",
              "        0.05810547, -0.18652344, -0.08007812, -0.2109375 , -0.16992188,\n",
              "       -0.34375   ,  0.07568359,  0.16894531, -0.15625   ,  0.1875    ,\n",
              "        0.2890625 , -0.125     , -0.03637695,  0.07324219,  0.02758789,\n",
              "        0.13085938,  0.08789062, -0.27148438, -0.11035156,  0.12988281,\n",
              "        0.03637695, -0.12255859,  0.22949219, -0.02197266,  0.06787109,\n",
              "       -0.24511719, -0.09033203, -0.26171875,  0.06738281,  0.0859375 ,\n",
              "       -0.09423828,  0.08447266,  0.09619141, -0.00537109,  0.04467773,\n",
              "        0.04345703,  0.36523438, -0.07177734,  0.00061798,  0.04052734,\n",
              "        0.0177002 ,  0.05004883, -0.25976562, -0.07226562, -0.00280762,\n",
              "       -0.14941406, -0.11621094,  0.03808594,  0.14160156,  0.32421875,\n",
              "        0.09082031, -0.10791016, -0.01733398,  0.13574219,  0.20507812,\n",
              "        0.11132812, -0.06054688, -0.05493164,  0.13476562, -0.06835938,\n",
              "        0.17871094,  0.03088379,  0.00982666, -0.18847656, -0.07519531,\n",
              "       -0.06787109,  0.20507812,  0.32617188,  0.44140625,  0.14746094,\n",
              "       -0.29101562, -0.19042969, -0.15625   , -0.27148438,  0.07080078,\n",
              "       -0.13769531, -0.04589844, -0.12451172, -0.17675781,  0.01904297,\n",
              "       -0.03369141, -0.09912109, -0.11132812, -0.03125   ,  0.05981445,\n",
              "       -0.03564453,  0.11279297, -0.04492188,  0.12890625,  0.1875    ,\n",
              "       -0.14453125,  0.07958984, -0.0402832 , -0.11621094,  0.09082031,\n",
              "        0.06542969, -0.06152344, -0.12597656,  0.125     ,  0.18359375,\n",
              "       -0.04223633, -0.1640625 , -0.21679688, -0.10400391,  0.19140625,\n",
              "        0.00448608,  0.08984375, -0.08300781, -0.04272461,  0.03759766,\n",
              "        0.10888672,  0.13769531, -0.28515625,  0.04492188,  0.03930664],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "0BRJTcBLX2Kn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3O5AAwp6X27n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b3c61619-da2c-4539-f76c-f91ee273733e"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "note that i am not using this pretarained model anymore beacuse there are many words \n",
        "herer that are not in the pretrained model. may be this is because those words didnt occur in the training corpus ie google news corpus.\n",
        "\n",
        "one such word is 'foster' some thing and this showed error for this.\n",
        "\n",
        "\n",
        "now after reading on net there are a couple of things which one can do but none of them makes sense to me.\n",
        "\n",
        "the first one is to leave thiose words which are not in the pre-trained vocab but actually those words are mlore importanrt as those words are specific to our task.\n",
        "\n",
        "the second option given on net is to use a fixed random vector to evry unknown word.\n",
        "now that in MY ppoint of view will lead to overfitting as we are making some useless pattern (fixed vector for every word).\n",
        "\n",
        "Anyways each of this approach can be easily tried and implemented if u want.\n",
        "\n",
        "'''"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nnote that i am not using this pretarained model anymore beacuse there are many words \\nherer that are not in the pretrained model. may be this is because those words didnt occur in the training corpus ie google news corpus.\\n\\none such word is 'foster' some thing and this showed error for this.\\n\\n\\nnow after reading on net there are a couple of things which one can do but none of them makes sense to me.\\n\\nthe first one is to leave thiose words which are not in the pre-trained vocab but actually those words are mlore importanrt as those words are specific to our task.\\n\\nthe second option given on net is to use a fixed random vector to evry unknown word.\\nnow that in MY ppoint of view will lead to overfitting as we are making some useless pattern (fixed vector for every word).\\n\\nAnyways each of this approach can be easily tried and implemented if u want.\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "JzRj9OrTX24r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r6NH2e6dX2-0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GHBfEB9cb92L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "882a2a9e-3052-4a41-f627-31ef7568eb75"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "now let us try to train the embeddings on ourc orpus like we did in imdb dataset.\n",
        "\n",
        "'''"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nnow let us try to train the embeddings on ourc orpus like we did in imdb dataset.\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "Sb-TrD30b94v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MDrGv3eyb98G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# pre-processing the data as needed by Gensim. (that 'sentences' arguement)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JWGXr1OZn2n4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "  text = re.sub(\"[^a-zA-Z]\",\" \",text)\n",
        "  # 3. Converting to lower case and splitting\n",
        "  word_tokens=text.lower().split()\n",
        "    \n",
        "    # 4. Remove stopwords\n",
        "  stop_words= set(stopwords.words(\"english\"))     \n",
        "  word_tokens= [w for w in word_tokens if not w in stop_words]\n",
        "    \n",
        "  cleaned_review=\" \".join(word_tokens)\n",
        "  return cleaned_review\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o99ZoKK4sPws",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# time taking cell.\n",
        " \n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "sentences=[]\n",
        "for text in train['text']:\n",
        "  # print(type(review))\n",
        "  sents=tokenizer.tokenize(str(text).strip())\n",
        "  for sent in sents:\n",
        "    cleaned_sent=clean_text(sent)\n",
        "    sentences.append(cleaned_sent.split())\n",
        " \n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hj4VZ1YH7fXr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " # print(len(sentences))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qRhHGRd1eXiU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# adding sentences from test set so that also see the vocabulary in test set.\n",
        "for text in test['text']:\n",
        "  # print(type(review))\n",
        "  sents=tokenizer.tokenize(str(text).strip())\n",
        "  for sent in sents:\n",
        "    cleaned_sent=clean_text(sent)\n",
        "    sentences.append(cleaned_sent.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5pELqyvEfLUx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff59e643-052f-491d-aae9-c01f89eeaf9d"
      },
      "cell_type": "code",
      "source": [
        "print(len(sentences))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s_TftxIrfLep",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1071
        },
        "outputId": "8fbcd5cd-031c-40d3-fd2f-08324dd6bf74"
      },
      "cell_type": "code",
      "source": [
        "test['text']"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Still, as I urged our leaving Ireland with suc...\n",
              "1       If a fire wanted fanning, it could readily be ...\n",
              "2       And when they had broken down the frail door t...\n",
              "3       While I was thinking how I should possibly man...\n",
              "4       I am not sure to what limit his knowledge may ...\n",
              "5       \"The thick and peculiar mist, or smoke, which ...\n",
              "6       That which is not matter, is not at all unless...\n",
              "7       I sought for repose although I did not hope fo...\n",
              "8       Upon the fourth day of the assassination, a pa...\n",
              "9              \"The tone metaphysical is also a good one.\n",
              "10      These, the offspring of a later period, stood ...\n",
              "11      What kept him from going with her and Brown Je...\n",
              "12      Persuading the widow that my connexion with he...\n",
              "13      When I arose trembling, I know not how much la...\n",
              "14      And by the shores of the river Zaire there is ...\n",
              "15      Idris heard of her mother's return with pleasure.\n",
              "16      I say this proudly, but with tears in my eyes ...\n",
              "17      But let us glance at the treatise Ah \"Ability ...\n",
              "18      \"What a place is this that you inhabit, my son...\n",
              "19      At his nod I took one of the latter and seated...\n",
              "20      No one doubted now that the mystery of this mu...\n",
              "21      But although, in one or two instances, arrests...\n",
              "22      Festivity, and even libertinism, became the or...\n",
              "23            For I am Iranon, who was a Prince in Aira.\"\n",
              "24      \"Gaze not on the star, dear, generous friend,\"...\n",
              "25      I am serious in asserting that my breath was e...\n",
              "26      The thing will haunt me, for who can say the e...\n",
              "27      Before each of the party lay a portion of a sk...\n",
              "28      If she had been bred in that sphere of life to...\n",
              "29      Or, if this mode of speech offend you, let me ...\n",
              "                              ...                        \n",
              "8362    Then again he distracted my thoughts from my s...\n",
              "8363    Upon the whole, whether happily or unhappily, ...\n",
              "8364    He was not allowed to finish this speech in tr...\n",
              "8365    His looks were wild with terror, and he spoke ...\n",
              "8366    By the quantity of provision which I had consu...\n",
              "8367    I hurled after the scoundrel these vehement wo...\n",
              "8368    Notwithstanding the hazardous object of our jo...\n",
              "8369    I felt the greatest eagerness to hear the prom...\n",
              "8370    But in the expression of the countenance, whic...\n",
              "8371    Its decorations were rich, yet tattered and an...\n",
              "8372    He directed my attention to some object agains...\n",
              "8373    Hey? Haow'd ye like to hear the haowlin' night...\n",
              "8374    She was buried not in a vault, but in an ordin...\n",
              "8375    In company with this sprightly and clever Gree...\n",
              "8376    In this unnerved in this pitiable condition I ...\n",
              "8377    He was a scoundrel, and I don't blame you for ...\n",
              "8378    But why should I dwell upon the incidents that...\n",
              "8379    In the streets were spears of long grass, and ...\n",
              "8380    When I first sought it, it was the love of vir...\n",
              "8381    But it is in matters beyond the limits of mere...\n",
              "8382    \"I may say an excellently well constructed house.\n",
              "8383    Across a covered bridge one sees a small villa...\n",
              "8384    You cannot take up a common newspaper in which...\n",
              "8385    Consoling myself with this reflection, I was m...\n",
              "8386    Yet we laughed and were merry in our proper wa...\n",
              "8387           All this is now the fitter for my purpose.\n",
              "8388                   I fixed myself on a wide solitude.\n",
              "8389    It is easily understood that what might improv...\n",
              "8390    Be this as it may, I now began to feel the ins...\n",
              "8391    Long winded, statistical, and drearily genealo...\n",
              "Name: text, Length: 8392, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "bg1eyknl8ad6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w2v_model=Word2Vec(sentences,size=300,window=10) # try to tune parameters."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xoBwP8fX8tEo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52817401-56f4-4db9-c209-d3dd352f7b56"
      },
      "cell_type": "code",
      "source": [
        "w2v_model.train(sentences,total_examples=len(sentences),epochs=10)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3292547, 3643640)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "5rHtlmLMXubX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9635d36a-d50d-4674-f053-cf4c07769530"
      },
      "cell_type": "code",
      "source": [
        "w2v_model"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.word2vec.Word2Vec at 0x7f511fa2cd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "ev-fPRsc9hik",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df['text']=df['text'].apply(clean_text)\n",
        "test['text']=test['text'].apply(clean_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6hMLEZaj9EnP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_doc_vectors(review):\n",
        "  doc_vec=np.zeros(300,dtype='float64') # initialize with zeros.\n",
        "  words=review.split()\n",
        "  for word in words: # iterate over all words.\n",
        "    if word in w2v_model.wv.vocab:\n",
        "      word_vec=w2v_model.wv.get_vector(word) # if word in vocab add its word vector to our doc vector\n",
        "      doc_vec=doc_vec+word_vec\n",
        "  return doc_vec/len(words)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "waaoHTmk9RGa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc_vectors_train=[]\n",
        "for review in df['text']:\n",
        "  val=make_doc_vectors(review)\n",
        "  doc_vectors_train.append(val)\n",
        "doc_vectors_train=np.array(doc_vectors_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_hPi9zteWXy3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "5233d7d5-c074-4e44-ce07-b39accfc8c07"
      },
      "cell_type": "code",
      "source": [
        "doc_vectors_train"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.06558332, -0.16856991,  0.2569005 , ...,  0.19822277,\n",
              "         0.4968006 , -0.21380241],\n",
              "       [-0.15052178,  0.03866731,  0.38813798, ...,  0.48441719,\n",
              "         0.54545498, -0.03405831],\n",
              "       [ 0.08852132, -0.14487087,  0.05528754, ..., -0.054616  ,\n",
              "         0.22446379, -0.11889364],\n",
              "       ...,\n",
              "       [ 0.02334363,  0.13543085,  0.11341397, ...,  0.23190454,\n",
              "         0.24117448,  0.0506095 ],\n",
              "       [-0.05008939, -0.26478426, -0.05355897, ...,  0.00123336,\n",
              "         0.2369076 , -0.03962618],\n",
              "       [ 0.02919731,  0.02457661, -0.01034997, ...,  0.03794237,\n",
              "         0.17442343,  0.04021765]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "metadata": {
        "id": "xIIQcbmz-GMU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "le=LabelEncoder()\n",
        "Y=le.fit_transform(df['author'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V5OhWVKR9yHl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "c419fe4f-f586-4b5c-c863-b74fae3616c3"
      },
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(doc_vectors_train,Y,test_size=0.2,random_state=42)\n",
        "np.isnan(x_train)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       ...,\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False],\n",
              "       [False, False, False, ..., False, False, False]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "metadata": {
        "id": "gZLpTYvaTAK0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "3d692075-8f7f-4701-c6f0-398f4d40406a"
      },
      "cell_type": "code",
      "source": [
        "doc_vectors_train"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.06558332, -0.16856991,  0.2569005 , ...,  0.19822277,\n",
              "         0.4968006 , -0.21380241],\n",
              "       [-0.15052178,  0.03866731,  0.38813798, ...,  0.48441719,\n",
              "         0.54545498, -0.03405831],\n",
              "       [ 0.08852132, -0.14487087,  0.05528754, ..., -0.054616  ,\n",
              "         0.22446379, -0.11889364],\n",
              "       ...,\n",
              "       [ 0.02334363,  0.13543085,  0.11341397, ...,  0.23190454,\n",
              "         0.24117448,  0.0506095 ],\n",
              "       [-0.05008939, -0.26478426, -0.05355897, ...,  0.00123336,\n",
              "         0.2369076 , -0.03962618],\n",
              "       [ 0.02919731,  0.02457661, -0.01034997, ...,  0.03794237,\n",
              "         0.17442343,  0.04021765]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "metadata": {
        "id": "ya9A8eoz-SMF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "outputId": "a1fb355a-49ec-478d-e235-a8c398b6c8ce"
      },
      "cell_type": "code",
      "source": [
        "clf=LogisticRegression()\n",
        "clf.fit(x_train,y_train)\n",
        "pred=clf.predict_proba(x_test)\n",
        "print(log_loss(y_test,pred))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-176e970a821b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1288\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1289\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    757\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 573\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "joARAArH-k8C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nan_x=np.isnan(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-rhG_IW1kRQV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "60eae457-3d25-4d3e-dbf4-7e27f87c617d"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "on running LR it is showing that input has nan values. i have checked everything but i am not able to figure out whats the cause.\n",
        "\n",
        "hence now i am leaving this as I know how to cretae the embeddings and how to represent document vectors using this method of averaged w2v vectors and also seen how to load and create pre-trained embeddings. this is takeway from this notebook.\n",
        "\n",
        "'''"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\non running LR it is showing that input has nan values. i have checked everything but i am not able to figure out whats the cause.\\n\\nhence now i am leaving this as I know how to cretae the embeddings and how to represent document vectors using this method of averaged w2v vectors and also seen how to load and create pre-trained embeddings. this is takeway from this notebook.\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "x4tNXh0Dm-b6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9cd88993-a3a7-444e-d4ae-23e8551a14a6"
      },
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15663, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "metadata": {
        "id": "Twypm7pLnAIb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}